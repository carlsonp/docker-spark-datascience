{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, pickle\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import *\n",
    "from IPython.core.display import display, HTML\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib plot sizes larger\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "\n",
    "conf = SparkConf().setAppName('Steam Random Forest Regressor').setMaster('spark://sparkmaster:7077')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g') # memory per executor\n",
    "SparkContext.setSystemProperty('spark.executor.cores', '6') # cores per executor\n",
    "SparkContext.setSystemProperty('spark.executor.instances', '3') # per worker (computer)\n",
    "\n",
    "# https://spark.apache.org/docs/3.0.0-preview/configuration.html#dynamic-allocation\n",
    "# https://stackoverflow.com/questions/26168254/how-to-set-amount-of-spark-executors\n",
    "# https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/\n",
    "\n",
    "# SparkContext.setSystemProperty(\"spark.shuffle.service.enabled\", \"True\") # required for dynamic allocation below\n",
    "# SparkContext.setSystemProperty(\"spark.dynamicAllocation.enabled\", \"True\")\n",
    "# SparkContext.setSystemProperty(\"spark.executor.cores\", \"4\")\n",
    "# SparkContext.setSystemProperty(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "# SparkContext.setSystemProperty(\"spark.dynamicAllocation.maxExecutors\", \"5\")\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '2g') # memory per executor\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '2g'),\n",
       " ('spark.driver.host', 'jupyterlab'),\n",
       " ('spark.app.id', 'app-20210419213454-0005'),\n",
       " ('spark.driver.port', '35941'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.app.name', 'Steam Random Forest Regressor'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.master', 'spark://sparkmaster:7077'),\n",
       " ('spark.executor.cores', '6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1618889694183'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"df.p\", \"rb\"))\n",
    "feature_list = pickle.load(open(\"feature_list.p\", \"rb\"))\n",
    "assembler = pickle.load(open(\"assembler.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(labelCol=\"days_until_discount\", featuresCol=\"features\")\n",
    "gbt = GBTRegressor(labelCol=\"days_until_discount\", featuresCol=\"features\")\n",
    "lr = LinearRegression(labelCol=\"days_until_discount\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "gbt_pipeline = Pipeline(stages=[assembler, gbt])\n",
    "linear_pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tweak these parameters more\n",
    "\n",
    "# Random Forest\n",
    "paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(rf.numTrees, [int(x) for x in np.linspace(start=10, stop=200, num=8)]) \\\n",
    ".addGrid(rf.maxDepth, [int(x) for x in np.linspace(start=1, stop=20, num=8)]) \\\n",
    ".build()\n",
    "#.addGrid(rf.maxBins, ?) \\ # should this also be set?\n",
    "\n",
    "# Gradient Boosted Tree\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(gbt.maxIter, [int(x) for x in np.linspace(start=5, stop=25, num=2)]) \\\n",
    ".addGrid(gbt.maxDepth, [int(x) for x in np.linspace(start=1, stop=20, num=8)]) \\\n",
    ".build()\n",
    "#.addGrid(gbt.maxBins, ?) \\ # should this also be set?\n",
    "\n",
    "# Linear Regression\n",
    "linear_paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    ".addGrid(lr.fitIntercept, [False, True]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator().setLabelCol(\"days_until_discount\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "gbt_crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                         estimatorParamMaps=gbt_paramGrid,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "linear_crossval = CrossValidator(estimator=linear_pipeline,\n",
    "                         estimatorParamMaps=linear_paramGrid,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = df.sample(frac=0.8)\n",
    "testData = df.drop(trainingData.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas dataframe to spark dataframe\n",
    "trainingData = sqlContext.createDataFrame(trainingData)\n",
    "testData = sqlContext.createDataFrame(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: look into partitions\n",
    "# https://luminousmen.com/post/spark-partitions\n",
    "# https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mutual information\n",
    "# https://www.kaggle.com/ryanholbrook/mutual-information\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "integer_df = df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "X = integer_df.copy()\n",
    "X.pop(\"appid\")\n",
    "y = X.pop(\"days_until_discount\")\n",
    "\n",
    "discrete_features = X.dtypes == int\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(10, 14))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "cvModel = crossval.fit(trainingData)\n",
    "endtime = time.time()\n",
    "\n",
    "print(\"Random Forest Training took: \" + str((endtime-starttime)/60) + \" minutes\")\n",
    "\n",
    "cvModel.write().overwrite().save(\"/work/steam-randomforest-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "gbt_model = gbt_crossval.fit(trainingData)\n",
    "endtime = time.time()\n",
    "\n",
    "print(\"Gradient Boosted Tree Training took: \" + str((endtime-starttime)/60) + \" minutes\")\n",
    "\n",
    "gbt_model.write().overwrite().save(\"/work/steam-gbt-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "linear_model = linear_crossval.fit(trainingData)\n",
    "endtime = time.time()\n",
    "\n",
    "print(\"Linear Regression Training took: \" + str((endtime-starttime)/60) + \" minutes\")\n",
    "\n",
    "linear_model.write().overwrite().save(\"/work/steam-linear-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel.transform(testData)\n",
    "gbt_predictions = gbt_model.transform(testData)\n",
    "linear_predictions = linear_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
