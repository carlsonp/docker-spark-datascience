{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.0.0-preview2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "20/06/13 15:35:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: scala/Function0$class\n",
      "\tat org.apache.livy.shaded.json4s.ThreadLocal.<init>(Formats.scala:311)\n",
      "\tat org.apache.livy.shaded.json4s.DefaultFormats$class.$init$(Formats.scala:318)\n",
      "\tat org.apache.livy.shaded.json4s.DefaultFormats$.<init>(Formats.scala:296)\n",
      "\tat org.apache.livy.shaded.json4s.DefaultFormats$.<clinit>(Formats.scala)\n",
      "\tat org.apache.livy.repl.Session.<init>(Session.scala:66)\n",
      "\tat org.apache.livy.repl.ReplDriver.initializeSparkEntries(ReplDriver.scala:41)\n",
      "\tat org.apache.livy.rsc.driver.RSCDriver.run(RSCDriver.java:337)\n",
      "\tat org.apache.livy.rsc.driver.RSCDriverBootstrapper.main(RSCDriverBootstrapper.java:93)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.ClassNotFoundException: scala.Function0$class\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\t... 20 more.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Spark%20Kernel.ipynb\n",
    "# https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>dead</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternating_Least_Squares.ipynb\n",
      "Spark-Pi-Test.ipynb\n",
      "SparkMagic-Test.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.0.0-preview2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "20/06/13 15:35:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: scala/Function0$class\n",
      "\tat org.apache.livy.shaded.json4s.ThreadLocal.<init>(Formats.scala:311)\n",
      "\tat org.apache.livy.shaded.json4s.DefaultFormats$class.$init$(Formats.scala:318)\n",
      "\tat org.apache.livy.shaded.json4s.DefaultFormats$.<init>(Formats.scala:296)\n",
      "\tat org.apache.livy.shaded.json4s.DefaultFormats$.<clinit>(Formats.scala)\n",
      "\tat org.apache.livy.repl.Session.<init>(Session.scala:66)\n",
      "\tat org.apache.livy.repl.ReplDriver.initializeSparkEntries(ReplDriver.scala:41)\n",
      "\tat org.apache.livy.rsc.driver.RSCDriver.run(RSCDriver.java:337)\n",
      "\tat org.apache.livy.rsc.driver.RSCDriverBootstrapper.main(RSCDriverBootstrapper.java:93)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.ClassNotFoundException: scala.Function0$class\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\t... 20 more"
     ]
    }
   ],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0ab965498849c096dfdcd99d0bb750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %spark [-c CONTEXT] [-s SESSION] [-o OUTPUT] [-q [QUIET]]\n",
       "             [-m SAMPLEMETHOD] [-n MAXROWS] [-r SAMPLEFRACTION] [-u URL]\n",
       "             [-a USER] [-p PASSWORD] [-t AUTH] [-l LANGUAGE] [-k [SKIP]]\n",
       "             [-i ID] [-e COERCE]\n",
       "             [command [command ...]]\n",
       "\n",
       "Magic to execute spark remotely.\n",
       "\n",
       "This magic allows you to create a Livy Scala or Python session against a Livy endpoint. Every session can\n",
       "be used to execute either Spark code or SparkSQL code by executing against the SQL context in the session.\n",
       "When the SQL context is used, the result will be a Pandas dataframe of a sample of the results.\n",
       "\n",
       "If invoked with no subcommand, the cell will be executed against the specified session.\n",
       "\n",
       "Subcommands\n",
       "-----------\n",
       "info\n",
       "    Display the available Livy sessions and other configurations for sessions.\n",
       "add\n",
       "    Add a Livy session given a session name (-s), language (-l), and endpoint credentials.\n",
       "    The -k argument, if present, will skip adding this session if it already exists.\n",
       "    e.g. `%spark add -s test -l python -u https://sparkcluster.net/livy -t Kerberos -a u -p -k`\n",
       "config\n",
       "    Override the livy session properties sent to Livy on session creation. All session creations will\n",
       "    contain these config settings from then on.\n",
       "    Expected value is a JSON key-value string to be sent as part of the Request Body for the POST /sessions\n",
       "    endpoint in Livy.\n",
       "    e.g. `%%spark config`\n",
       "         `{\"driverMemory\":\"1000M\", \"executorCores\":4}`\n",
       "run\n",
       "    Run Spark code against a session.\n",
       "    e.g. `%%spark -s testsession` will execute the cell code against the testsession previously created\n",
       "    e.g. `%%spark -s testsession -c sql` will execute the SQL code against the testsession previously created\n",
       "    e.g. `%%spark -s testsession -c sql -o my_var` will execute the SQL code against the testsession\n",
       "             previously created and store the pandas dataframe created in the my_var variable in the\n",
       "             Python environment.\n",
       "logs\n",
       "    Returns the logs for a given session.\n",
       "    e.g. `%spark logs -s testsession` will return the logs for the testsession previously created\n",
       "delete\n",
       "    Delete a Livy session.\n",
       "    e.g. `%spark delete -s defaultlivy`\n",
       "cleanup\n",
       "    Delete all Livy sessions created by the notebook. No arguments required.\n",
       "    e.g. `%spark cleanup`\n",
       "\n",
       "positional arguments:\n",
       "  command               Commands to execute.\n",
       "\n",
       "optional arguments:\n",
       "  -c CONTEXT, --context CONTEXT\n",
       "                        Context to use: 'spark' for spark and 'sql' for sql\n",
       "                        queries. Default is 'spark'.\n",
       "  -s SESSION, --session SESSION\n",
       "                        The name of the Livy session to use.\n",
       "  -o OUTPUT, --output OUTPUT\n",
       "                        If present, output when using SQL queries will be\n",
       "                        stored in this variable.\n",
       "  -q <[QUIET]>, --quiet <[QUIET]>\n",
       "                        Do not display visualizations on SQL queries\n",
       "  -m SAMPLEMETHOD, --samplemethod SAMPLEMETHOD\n",
       "                        Sample method for SQL queries: either take or sample\n",
       "  -n MAXROWS, --maxrows MAXROWS\n",
       "                        Maximum number of rows that will be pulled back from\n",
       "                        the server for SQL queries\n",
       "  -r SAMPLEFRACTION, --samplefraction SAMPLEFRACTION\n",
       "                        Sample fraction for sampling from SQL queries\n",
       "  -u URL, --url URL     URL for Livy endpoint\n",
       "  -a USER, --user USER  Username for HTTP access to Livy endpoint\n",
       "  -p PASSWORD, --password PASSWORD\n",
       "                        Password for HTTP access to Livy endpoint\n",
       "  -t AUTH, --auth AUTH  Auth type for HTTP access to Livy endpoint. [Kerberos,\n",
       "                        None, Basic Auth]\n",
       "  -l LANGUAGE, --language LANGUAGE\n",
       "                        Language for Livy session; one of python, scala, r\n",
       "  -k <[SKIP]>, --skip <[SKIP]>\n",
       "                        Skip adding session if it already exists\n",
       "  -i ID, --id ID        Session ID\n",
       "  -e COERCE, --coerce COERCE\n",
       "                        Whether to automatically coerce the types (default,\n",
       "                        pass True if being explicit) of the dataframe or not\n",
       "                        (pass False)\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.8/dist-packages/sparkmagic/livyclientlib/exceptions.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "You need to have at least 1 client created to execute commands.\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
